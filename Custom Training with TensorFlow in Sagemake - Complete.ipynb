{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training with TensorFlow in Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "urls = ['http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz',\n",
    "        'http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz']\n",
    "\n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(data_dir, download_dir):\n",
    "    for url in urls:\n",
    "        target_file = url.split('/')[-1]\n",
    "        if target_file not in os.listdir(download_dir):\n",
    "            print('Downloading', url)\n",
    "            urllib.request.urlretrieve(url, os.path.join(download_dir, target_file))\n",
    "            tf = tarfile.open(url.split('/')[-1])\n",
    "            tf.extractall(data_dir)\n",
    "        else:\n",
    "            print('Already downloaded', url)\n",
    "\n",
    "def get_annotations(file_path, annotations={}):\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        rows = f.read().splitlines()\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        image_name, _, _, _ = row.split(' ')\n",
    "        class_name = image_name.split('_')[:-1]\n",
    "        class_name = '_'.join(class_name)\n",
    "        image_name = image_name + '.jpg'\n",
    "        \n",
    "        annotations[image_name] = 'cat' if class_name[0] != class_name[0].lower() else 'dog'\n",
    "    \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "download_and_extract('data', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = get_annotations('data/annotations/trainval.txt')\n",
    "annotations = get_annotations('data/annotations/test.txt', annotations)\n",
    "\n",
    "total_count = len(annotations.keys())\n",
    "print('Total examples', total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(annotations.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cat', 'dog']\n",
    "sets = ['train', 'validation']\n",
    "root_dir = 'custom_data'\n",
    "\n",
    "if not os.path.isdir(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "    \n",
    "for set_name in sets:\n",
    "    if not os.path.isdir(os.path.join(root_dir, set_name)):\n",
    "        os.mkdir(os.path.join(root_dir, set_name))\n",
    "    for class_name in classes:\n",
    "        folder = os.path.join(root_dir, set_name, class_name)\n",
    "        if not os.path.isdir(folder):\n",
    "            os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the files to correct set/ class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, class_name in annotations.items():\n",
    "    target_set = 'validation' if random.randint(0, 99) < 20 else 'train'\n",
    "    target_path = os.path.join(root_dir, target_set, class_name, image)\n",
    "    shutil.copy(os.path.join('data/images/', image), target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_counts = {\n",
    "    'train': 0,\n",
    "    'validation': 0\n",
    "}\n",
    "\n",
    "for set_name in sets:\n",
    "    for class_name in classes:\n",
    "        path = os.path.join(root_dir, set_name, class_name)\n",
    "        count = len(os.listdir(path))\n",
    "        print(path, 'has', count, 'images')\n",
    "        sets_counts[set_name] += count\n",
    "\n",
    "print(sets_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script - Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False, weights='imagenet',\n",
    "                                                       pooling='avg', input_shape=(128, 128, 3)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.layers[0].trainable = False\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script - Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "def create_data_generators(root_dir, batch_size):\n",
    "    train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=[0.8, 1.2],\n",
    "        rotation_range=20\n",
    "    ).flow_from_directory(\n",
    "        os.path.join(root_dir, 'train'),\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    ).flow_from_directory(\n",
    "        os.path.join(root_dir, 'validation'),\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    return train_data_generator, val_data_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script - Putting it Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--steps', type=int, default=int(5873/16))\n",
    "    parser.add_argument('--val_steps', type=int, default=(1476/16))\n",
    "\n",
    "    # input data and model directories\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    local_output_dir = args.sm_model_dir\n",
    "    local_root_dir = args.train\n",
    "    batch_size = args.batch_size\n",
    "    \n",
    "    model = create_model()\n",
    "    train_gen, val_gen = create_data_generators(local_root_dir, batch_size)\n",
    "    \n",
    "    _ = model.fit(\n",
    "        train_gen,\n",
    "        epochs=args.epochs,\n",
    "        steps_per_epoch=args.steps,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=args.val_steps\n",
    "    )\n",
    "    \n",
    "    model.save(os.path.join(local_output_dir, 'model', '1'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket_name = '--Enter your bucket name--'\n",
    "\n",
    "print('Uploading to S3..')\n",
    "s3_data_path = sagemaker_session.upload_data(path=root_dir, bucket=bucket_name, key_prefix='data')\n",
    "\n",
    "print('Uploaded to', s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with TensorFlow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "pets_estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    framework_version='2.1.0 (change according to your version)',\n",
    "    py_version='py3 (for python 3.x)',\n",
    "    output_path='s3://--enter your Bucketname here--/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_estimator.fit(s3_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_predictor = pets_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "print('\\nModel Deployed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dir = 'custom_data/validation/cat/'\n",
    "cat_images = [os.path.join(cat_dir, x) for x in os.listdir(cat_dir)]\n",
    "print(cat_images[0])\n",
    "\n",
    "dog_dir = 'custom_data/validation/dog/'\n",
    "dog_images = [os.path.join(dog_dir, x) for x in os.listdir(dog_dir)]\n",
    "print(dog_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(128, 128))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    results = pets_predictor.predict(img)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = cat_images[0]\n",
    "results = get_pred(image_path)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = int(np.squeeze(results['predictions']) > 0.5)\n",
    "print('Predicted class_id:', class_id, 'with class_name:', classes[class_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(pets_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
